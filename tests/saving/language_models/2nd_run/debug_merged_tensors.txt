Tensors being saved to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors:
  model.layers.20.input_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.20.mlp.down_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 8192])
  model.layers.20.post_attention_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.21.input_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.21.mlp.down_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 8192])
  model.layers.21.mlp.gate_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.21.mlp.up_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.21.post_attention_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.21.self_attn.k_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.21.self_attn.o_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.21.self_attn.q_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.21.self_attn.v_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.22.input_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.22.mlp.down_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 8192])
  model.layers.22.mlp.gate_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.22.mlp.up_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.22.post_attention_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.22.self_attn.k_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.22.self_attn.o_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.22.self_attn.q_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.22.self_attn.v_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.23.input_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.23.mlp.down_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 8192])
  model.layers.23.mlp.gate_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.23.mlp.up_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.23.post_attention_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.23.self_attn.k_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.23.self_attn.o_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.23.self_attn.q_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.23.self_attn.v_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.24.input_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.24.mlp.down_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 8192])
  model.layers.24.mlp.gate_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.24.mlp.up_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.24.post_attention_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.24.self_attn.k_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.24.self_attn.o_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.24.self_attn.q_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.24.self_attn.v_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.25.input_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.25.mlp.down_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 8192])
  model.layers.25.mlp.gate_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.25.mlp.up_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.25.post_attention_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.25.self_attn.k_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.25.self_attn.o_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.25.self_attn.q_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.25.self_attn.v_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.26.input_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.26.mlp.down_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 8192])
  model.layers.26.mlp.gate_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.26.mlp.up_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.26.post_attention_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.26.self_attn.k_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.26.self_attn.o_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.26.self_attn.q_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.26.self_attn.v_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.27.input_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.27.mlp.down_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 8192])
  model.layers.27.mlp.gate_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.27.mlp.up_proj.weight: dtype=torch.bfloat16, shape=torch.Size([8192, 3072])
  model.layers.27.post_attention_layernorm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
  model.layers.27.self_attn.k_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.layers.27.self_attn.o_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.27.self_attn.q_proj.weight: dtype=torch.bfloat16, shape=torch.Size([3072, 3072])
  model.layers.27.self_attn.v_proj.weight: dtype=torch.bfloat16, shape=torch.Size([1024, 3072])
  model.norm.weight: dtype=torch.bfloat16, shape=torch.Size([3072])
