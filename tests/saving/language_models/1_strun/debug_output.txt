🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
INFO 09-04 13:11:09 [__init__.py:241] Automatically detected platform cuda.
🦥 Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.0. vLLM: 0.10.1.1.
   \\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.209 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
dtype of dtype before calling split is <class 'torch.dtype'>
dtype of string_dtype is <class 'str'>
dtype of dtype before calling split is <class 'torch.dtype'>
dtype of string_dtype is <class 'str'>
dtype of dtype before calling split is <class 'torch.dtype'>
dtype of string_dtype is <class 'str'>
dtype of dtype before calling split is <class 'torch.dtype'>
dtype of string_dtype is <class 'str'>
  0%|          | 0/518 [00:00<?, ?it/s]  0%|          | 1/518 [00:00<04:34,  1.89it/s]  1%|          | 3/518 [00:00<01:35,  5.41it/s]  1%|          | 5/518 [00:00<01:02,  8.26it/s]  1%|▏         | 7/518 [00:00<00:46, 10.96it/s]  2%|▏         | 9/518 [00:00<00:39, 12.87it/s]  2%|▏         | 11/518 [00:01<00:34, 14.54it/s]  3%|▎         | 13/518 [00:01<00:32, 15.56it/s]  3%|▎         | 15/518 [00:01<00:30, 16.58it/s]  3%|▎         | 17/518 [00:01<00:29, 16.72it/s]  4%|▎         | 19/518 [00:01<00:28, 17.34it/s]  4%|▍         | 22/518 [00:02<00:57,  8.69it/s]  5%|▍         | 24/518 [00:02<00:48, 10.21it/s]  5%|▌         | 26/518 [00:02<00:42, 11.65it/s]  6%|▌         | 29/518 [00:02<00:34, 14.14it/s]  6%|▌         | 31/518 [00:02<00:33, 14.70it/s]  7%|▋         | 34/518 [00:02<00:29, 16.56it/s]  7%|▋         | 37/518 [00:02<00:27, 17.26it/s]  8%|▊         | 40/518 [00:03<00:26, 18.31it/s]  8%|▊         | 43/518 [00:03<00:24, 19.14it/s]  9%|▉         | 46/518 [00:03<00:24, 19.41it/s]  9%|▉         | 48/518 [00:03<00:24, 19.54it/s] 10%|▉         | 50/518 [00:03<00:24, 19.30it/s] 10%|█         | 52/518 [00:03<00:23, 19.43it/s] 11%|█         | 55/518 [00:03<00:22, 20.27it/s] 11%|█         | 58/518 [00:03<00:22, 20.07it/s] 12%|█▏        | 61/518 [00:04<00:22, 20.23it/s] 12%|█▏        | 64/518 [00:04<00:22, 20.31it/s] 13%|█▎        | 67/518 [00:04<00:22, 20.29it/s] 14%|█▎        | 70/518 [00:04<00:21, 20.46it/s] 14%|█▍        | 73/518 [00:04<00:21, 20.35it/s] 15%|█▍        | 76/518 [00:04<00:21, 20.43it/s] 15%|█▌        | 79/518 [00:04<00:21, 20.78it/s] 16%|█▌        | 82/518 [00:05<00:21, 20.66it/s] 16%|█▋        | 85/518 [00:05<00:21, 20.49it/s] 17%|█▋        | 88/518 [00:05<00:21, 20.07it/s] 18%|█▊        | 91/518 [00:05<00:21, 20.21it/s] 18%|█▊        | 94/518 [00:05<00:21, 20.12it/s] 19%|█▊        | 97/518 [00:05<00:20, 20.51it/s] 19%|█▉        | 100/518 [00:05<00:20, 20.57it/s] 20%|█▉        | 103/518 [00:06<00:19, 20.78it/s] 20%|██        | 106/518 [00:06<00:19, 20.81it/s] 21%|██        | 109/518 [00:06<00:19, 20.86it/s] 22%|██▏       | 112/518 [00:06<00:19, 20.45it/s] 22%|██▏       | 115/518 [00:06<00:19, 21.00it/s] 23%|██▎       | 118/518 [00:06<00:19, 20.84it/s] 23%|██▎       | 121/518 [00:06<00:18, 21.09it/s] 24%|██▍       | 124/518 [00:07<00:18, 21.41it/s] 25%|██▍       | 127/518 [00:07<00:18, 21.34it/s] 25%|██▌       | 130/518 [00:07<00:18, 21.35it/s] 26%|██▌       | 133/518 [00:07<00:18, 21.04it/s] 26%|██▋       | 136/518 [00:07<00:18, 21.14it/s] 27%|██▋       | 139/518 [00:07<00:18, 21.04it/s] 27%|██▋       | 142/518 [00:07<00:17, 21.12it/s] 28%|██▊       | 145/518 [00:08<00:17, 21.32it/s] 29%|██▊       | 148/518 [00:08<00:17, 21.46it/s] 29%|██▉       | 151/518 [00:08<00:17, 20.64it/s] 30%|██▉       | 154/518 [00:08<00:17, 21.34it/s] 30%|███       | 157/518 [00:08<00:16, 21.55it/s] 31%|███       | 160/518 [00:08<00:16, 21.58it/s] 31%|███▏      | 163/518 [00:08<00:16, 21.51it/s] 32%|███▏      | 166/518 [00:09<00:16, 21.14it/s] 33%|███▎      | 169/518 [00:09<00:16, 21.06it/s] 33%|███▎      | 172/518 [00:09<00:16, 21.28it/s] 34%|███▍      | 175/518 [00:09<00:16, 21.26it/s] 34%|███▍      | 178/518 [00:09<00:16, 21.16it/s] 35%|███▍      | 181/518 [00:09<00:15, 21.39it/s] 36%|███▌      | 184/518 [00:09<00:15, 21.50it/s] 36%|███▌      | 187/518 [00:10<00:15, 21.80it/s] 37%|███▋      | 190/518 [00:10<00:15, 21.74it/s] 37%|███▋      | 193/518 [00:10<00:14, 21.83it/s] 38%|███▊      | 196/518 [00:10<00:14, 22.15it/s] 38%|███▊      | 199/518 [00:10<00:14, 21.74it/s] 39%|███▉      | 202/518 [00:10<00:14, 21.68it/s] 40%|███▉      | 205/518 [00:10<00:14, 20.96it/s] 40%|████      | 208/518 [00:11<00:14, 21.20it/s] 41%|████      | 211/518 [00:11<00:14, 21.40it/s] 41%|████▏     | 214/518 [00:11<00:14, 21.49it/s] 42%|████▏     | 217/518 [00:11<00:13, 21.72it/s] 42%|████▏     | 220/518 [00:11<00:13, 21.89it/s] 43%|████▎     | 223/518 [00:11<00:13, 21.54it/s] 44%|████▎     | 226/518 [00:11<00:13, 21.35it/s] 44%|████▍     | 229/518 [00:12<00:14, 20.38it/s] 45%|████▍     | 232/518 [00:12<00:14, 20.31it/s] 45%|████▌     | 235/518 [00:12<00:13, 20.85it/s] 46%|████▌     | 238/518 [00:12<00:13, 20.48it/s] 47%|████▋     | 241/518 [00:12<00:13, 20.89it/s] 47%|████▋     | 244/518 [00:13<00:21, 12.51it/s] 48%|████▊     | 247/518 [00:13<00:19, 14.21it/s] 48%|████▊     | 250/518 [00:13<00:17, 15.64it/s] 49%|████▉     | 253/518 [00:13<00:15, 17.12it/s] 49%|████▉     | 256/518 [00:13<00:14, 18.39it/s] 50%|█████     | 259/518 [00:13<00:13, 19.25it/s] 51%|█████     | 262/518 [00:13<00:12, 20.43it/s] 51%|█████     | 265/518 [00:14<00:12, 21.03it/s] 52%|█████▏    | 268/518 [00:14<00:11, 21.00it/s] 52%|█████▏    | 271/518 [00:14<00:11, 21.41it/s] 53%|█████▎    | 274/518 [00:14<00:11, 21.68it/s] 53%|█████▎    | 277/518 [00:14<00:11, 21.78it/s] 54%|█████▍    | 280/518 [00:14<00:10, 21.92it/s] 55%|█████▍    | 283/518 [00:14<00:10, 22.11it/s] 55%|█████▌    | 286/518 [00:15<00:10, 21.87it/s] 56%|█████▌    | 289/518 [00:15<00:10, 21.96it/s] 56%|█████▋    | 292/518 [00:15<00:10, 21.79it/s] 57%|█████▋    | 295/518 [00:15<00:10, 22.16it/s] 58%|█████▊    | 298/518 [00:15<00:09, 22.34it/s] 58%|█████▊    | 301/518 [00:15<00:09, 21.89it/s] 59%|█████▊    | 304/518 [00:15<00:09, 21.73it/s] 59%|█████▉    | 307/518 [00:15<00:09, 21.72it/s] 60%|█████▉    | 310/518 [00:16<00:09, 21.61it/s] 60%|██████    | 313/518 [00:16<00:09, 21.83it/s] 61%|██████    | 316/518 [00:16<00:09, 22.35it/s] 62%|██████▏   | 319/518 [00:16<00:08, 22.75it/s] 62%|██████▏   | 322/518 [00:16<00:09, 19.82it/s] 63%|██████▎   | 325/518 [00:16<00:09, 20.51it/s] 63%|██████▎   | 328/518 [00:16<00:09, 20.61it/s] 64%|██████▍   | 331/518 [00:17<00:08, 20.97it/s] 64%|██████▍   | 334/518 [00:17<00:08, 21.26it/s] 65%|██████▌   | 337/518 [00:17<00:08, 21.24it/s] 66%|██████▌   | 340/518 [00:17<00:08, 20.91it/s] 66%|██████▌   | 343/518 [00:17<00:08, 20.98it/s] 67%|██████▋   | 346/518 [00:17<00:08, 20.83it/s] 67%|██████▋   | 349/518 [00:17<00:07, 21.34it/s] 68%|██████▊   | 352/518 [00:18<00:07, 21.30it/s] 69%|██████▊   | 355/518 [00:18<00:07, 21.12it/s] 69%|██████▉   | 358/518 [00:18<00:07, 21.10it/s] 70%|██████▉   | 361/518 [00:18<00:07, 21.33it/s] 70%|███████   | 364/518 [00:18<00:07, 21.69it/s] 71%|███████   | 367/518 [00:18<00:06, 21.61it/s] 71%|███████▏  | 370/518 [00:18<00:06, 21.68it/s] 72%|███████▏  | 373/518 [00:19<00:06, 21.89it/s] 73%|███████▎  | 376/518 [00:19<00:06, 21.99it/s] 73%|███████▎  | 379/518 [00:19<00:06, 22.25it/s] 74%|███████▎  | 382/518 [00:19<00:06, 22.36it/s] 74%|███████▍  | 385/518 [00:19<00:06, 21.90it/s] 75%|███████▍  | 388/518 [00:19<00:05, 21.74it/s] 75%|███████▌  | 391/518 [00:19<00:05, 21.93it/s] 76%|███████▌  | 394/518 [00:20<00:05, 21.97it/s] 77%|███████▋  | 397/518 [00:20<00:05, 22.29it/s] 77%|███████▋  | 400/518 [00:20<00:05, 22.09it/s] 78%|███████▊  | 403/518 [00:20<00:05, 22.43it/s] 78%|███████▊  | 406/518 [00:20<00:05, 21.96it/s] 79%|███████▉  | 409/518 [00:20<00:04, 22.02it/s] 80%|███████▉  | 412/518 [00:20<00:04, 21.82it/s] 80%|████████  | 415/518 [00:20<00:04, 22.08it/s] 81%|████████  | 418/518 [00:21<00:04, 22.17it/s] 81%|████████▏ | 421/518 [00:21<00:04, 21.54it/s] 82%|████████▏ | 424/518 [00:21<00:04, 21.59it/s] 82%|████████▏ | 427/518 [00:21<00:04, 21.60it/s] 83%|████████▎ | 430/518 [00:21<00:04, 21.91it/s] 84%|████████▎ | 433/518 [00:21<00:03, 21.87it/s] 84%|████████▍ | 436/518 [00:21<00:03, 22.37it/s] 85%|████████▍ | 439/518 [00:22<00:03, 21.98it/s] 85%|████████▌ | 442/518 [00:22<00:03, 21.76it/s] 86%|████████▌ | 445/518 [00:22<00:03, 21.59it/s] 86%|████████▋ | 448/518 [00:22<00:03, 21.88it/s] 87%|████████▋ | 451/518 [00:22<00:03, 22.01it/s] 88%|████████▊ | 454/518 [00:22<00:02, 21.86it/s] 88%|████████▊ | 457/518 [00:22<00:02, 22.40it/s] 89%|████████▉ | 460/518 [00:23<00:02, 22.37it/s] 89%|████████▉ | 463/518 [00:23<00:02, 22.13it/s] 90%|████████▉ | 466/518 [00:23<00:02, 22.44it/s] 91%|█████████ | 469/518 [00:23<00:02, 22.21it/s] 91%|█████████ | 472/518 [00:23<00:02, 21.98it/s] 92%|█████████▏| 475/518 [00:23<00:01, 22.34it/s] 92%|█████████▏| 478/518 [00:23<00:01, 22.42it/s] 93%|█████████▎| 481/518 [00:23<00:01, 22.18it/s] 93%|█████████▎| 484/518 [00:24<00:01, 22.09it/s] 94%|█████████▍| 487/518 [00:24<00:01, 22.23it/s] 95%|█████████▍| 490/518 [00:24<00:01, 22.52it/s] 95%|█████████▌| 493/518 [00:24<00:01, 22.41it/s] 96%|█████████▌| 496/518 [00:24<00:01, 21.97it/s] 96%|█████████▋| 499/518 [00:24<00:00, 22.59it/s] 97%|█████████▋| 502/518 [00:24<00:00, 22.68it/s] 97%|█████████▋| 505/518 [00:25<00:00, 22.53it/s] 98%|█████████▊| 508/518 [00:25<00:00, 22.42it/s] 99%|█████████▊| 511/518 [00:25<00:00, 22.26it/s] 99%|█████████▉| 514/518 [00:25<00:00, 21.97it/s]100%|█████████▉| 517/518 [00:25<00:00, 22.70it/s]100%|██████████| 518/518 [00:25<00:00, 20.22it/s]
Unsloth 2025.9.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 9,846 | Num Epochs = 1 | Total steps = 10
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:01<00:12,  1.34s/it] 20%|██        | 2/10 [00:02<00:09,  1.18s/it] 30%|███       | 3/10 [00:03<00:07,  1.03s/it] 40%|████      | 4/10 [00:04<00:05,  1.06it/s] 50%|█████     | 5/10 [00:04<00:04,  1.12it/s] 60%|██████    | 6/10 [00:05<00:03,  1.17it/s] 70%|███████   | 7/10 [00:06<00:02,  1.16it/s] 80%|████████  | 8/10 [00:07<00:01,  1.19it/s] 90%|█████████ | 9/10 [00:08<00:00,  1.20it/s]100%|██████████| 10/10 [00:08<00:00,  1.21it/s]                                               100%|██████████| 10/10 [00:09<00:00,  1.21it/s]100%|██████████| 10/10 [00:09<00:00,  1.04it/s]
Unsloth: Will smartly offload gradients to save VRAM!
{'train_runtime': 9.6292, 'train_samples_per_second': 8.308, 'train_steps_per_second': 1.039, 'train_loss': 1.5299745559692384, 'epoch': 0.01}
  0%|          | 0/518 [00:00<?, ?it/s]  0%|          | 2/518 [00:00<00:26, 19.21it/s]  1%|          | 4/518 [00:00<00:27, 19.02it/s]  1%|          | 6/518 [00:00<00:26, 19.00it/s]  2%|▏         | 8/518 [00:00<00:26, 19.36it/s]  2%|▏         | 10/518 [00:00<00:26, 19.23it/s]  3%|▎         | 13/518 [00:00<00:25, 19.46it/s]  3%|▎         | 16/518 [00:00<00:25, 19.67it/s]  3%|▎         | 18/518 [00:00<00:25, 19.75it/s]  4%|▍         | 21/518 [00:01<00:24, 20.12it/s]  5%|▍         | 24/518 [00:01<00:39, 12.38it/s]  5%|▌         | 26/518 [00:01<00:36, 13.45it/s]  6%|▌         | 29/518 [00:01<00:31, 15.38it/s]  6%|▌         | 31/518 [00:01<00:30, 16.23it/s]  7%|▋         | 34/518 [00:01<00:27, 17.57it/s]  7%|▋         | 37/518 [00:02<00:26, 18.28it/s]  8%|▊         | 40/518 [00:02<00:25, 18.97it/s]  8%|▊         | 43/518 [00:02<00:24, 19.25it/s]  9%|▊         | 45/518 [00:02<00:24, 19.35it/s]  9%|▉         | 47/518 [00:02<00:24, 19.37it/s]  9%|▉         | 49/518 [00:02<00:24, 19.01it/s] 10%|▉         | 51/518 [00:02<00:24, 19.23it/s] 10%|█         | 54/518 [00:02<00:23, 19.58it/s] 11%|█         | 57/518 [00:03<00:23, 19.89it/s] 12%|█▏        | 60/518 [00:03<00:22, 20.00it/s] 12%|█▏        | 62/518 [00:03<00:22, 19.97it/s] 13%|█▎        | 65/518 [00:03<00:22, 19.92it/s] 13%|█▎        | 68/518 [00:03<00:22, 19.81it/s] 14%|█▎        | 71/518 [00:03<00:22, 19.69it/s] 14%|█▍        | 73/518 [00:03<00:22, 19.63it/s] 14%|█▍        | 75/518 [00:04<00:22, 19.47it/s] 15%|█▍        | 77/518 [00:04<00:22, 19.57it/s] 15%|█▌        | 79/518 [00:04<00:22, 19.52it/s] 16%|█▌        | 82/518 [00:04<00:21, 19.84it/s] 16%|█▋        | 85/518 [00:04<00:21, 20.09it/s] 17%|█▋        | 88/518 [00:04<00:21, 20.21it/s] 18%|█▊        | 91/518 [00:04<00:21, 19.84it/s] 18%|█▊        | 93/518 [00:04<00:21, 19.68it/s] 19%|█▊        | 96/518 [00:05<00:21, 19.79it/s] 19%|█▉        | 99/518 [00:05<00:20, 19.99it/s] 19%|█▉        | 101/518 [00:05<00:20, 19.86it/s] 20%|██        | 104/518 [00:05<00:20, 19.88it/s] 20%|██        | 106/518 [00:05<00:20, 19.79it/s] 21%|██        | 108/518 [00:05<00:20, 19.57it/s] 21%|██        | 110/518 [00:05<00:20, 19.65it/s] 22%|██▏       | 113/518 [00:05<00:20, 19.69it/s] 22%|██▏       | 116/518 [00:06<00:20, 19.80it/s] 23%|██▎       | 118/518 [00:06<00:20, 19.76it/s] 23%|██▎       | 121/518 [00:06<00:19, 19.95it/s] 24%|██▍       | 124/518 [00:06<00:19, 19.91it/s] 24%|██▍       | 126/518 [00:06<00:19, 19.90it/s] 25%|██▍       | 129/518 [00:06<00:19, 20.21it/s] 25%|██▌       | 132/518 [00:06<00:19, 20.04it/s] 26%|██▌       | 135/518 [00:07<00:19, 20.04it/s] 27%|██▋       | 138/518 [00:07<00:18, 20.09it/s] 27%|██▋       | 141/518 [00:07<00:18, 20.09it/s] 28%|██▊       | 144/518 [00:07<00:18, 20.03it/s] 28%|██▊       | 147/518 [00:07<00:18, 19.98it/s] 29%|██▉       | 150/518 [00:07<00:18, 20.00it/s] 30%|██▉       | 153/518 [00:07<00:18, 20.18it/s] 30%|███       | 156/518 [00:08<00:18, 19.97it/s] 31%|███       | 158/518 [00:08<00:18, 19.73it/s] 31%|███       | 160/518 [00:08<00:18, 19.62it/s] 31%|███▏      | 162/518 [00:08<00:18, 19.44it/s] 32%|███▏      | 164/518 [00:08<00:18, 19.35it/s] 32%|███▏      | 166/518 [00:08<00:18, 19.34it/s] 32%|███▏      | 168/518 [00:08<00:18, 19.36it/s] 33%|███▎      | 170/518 [00:08<00:17, 19.50it/s] 33%|███▎      | 172/518 [00:08<00:17, 19.57it/s] 34%|███▎      | 174/518 [00:09<00:17, 19.18it/s] 34%|███▍      | 177/518 [00:09<00:17, 19.74it/s] 35%|███▍      | 179/518 [00:09<00:17, 19.14it/s] 35%|███▍      | 181/518 [00:09<00:17, 19.34it/s] 35%|███▌      | 183/518 [00:09<00:17, 19.47it/s] 36%|███▌      | 186/518 [00:09<00:16, 19.82it/s] 36%|███▋      | 188/518 [00:09<00:16, 19.75it/s] 37%|███▋      | 191/518 [00:09<00:16, 20.20it/s] 37%|███▋      | 194/518 [00:10<00:16, 19.76it/s] 38%|███▊      | 196/518 [00:10<00:16, 19.77it/s] 38%|███▊      | 198/518 [00:10<00:16, 19.83it/s] 39%|███▉      | 201/518 [00:10<00:15, 19.87it/s] 39%|███▉      | 203/518 [00:10<00:16, 19.60it/s] 40%|███▉      | 205/518 [00:10<00:16, 19.38it/s] 40%|████      | 208/518 [00:10<00:15, 19.39it/s] 41%|████      | 210/518 [00:10<00:15, 19.43it/s] 41%|████      | 213/518 [00:11<00:15, 19.84it/s] 42%|████▏     | 216/518 [00:11<00:15, 20.12it/s] 42%|████▏     | 219/518 [00:11<00:14, 20.02it/s] 43%|████▎     | 222/518 [00:11<00:14, 20.08it/s] 43%|████▎     | 225/518 [00:11<00:14, 19.75it/s] 44%|████▍     | 228/518 [00:11<00:14, 19.46it/s] 44%|████▍     | 230/518 [00:11<00:14, 19.48it/s] 45%|████▍     | 232/518 [00:11<00:14, 19.59it/s] 45%|████▌     | 235/518 [00:12<00:14, 19.86it/s] 46%|████▌     | 238/518 [00:12<00:14, 19.80it/s] 47%|████▋     | 241/518 [00:12<00:13, 20.03it/s] 47%|████▋     | 244/518 [00:12<00:23, 11.85it/s] 47%|████▋     | 246/518 [00:13<00:21, 12.92it/s] 48%|████▊     | 248/518 [00:13<00:19, 14.08it/s] 48%|████▊     | 251/518 [00:13<00:16, 15.86it/s] 49%|████▉     | 253/518 [00:13<00:16, 16.52it/s] 49%|████▉     | 256/518 [00:13<00:14, 17.69it/s] 50%|█████     | 259/518 [00:13<00:14, 18.43it/s] 51%|█████     | 262/518 [00:13<00:13, 19.13it/s] 51%|█████     | 265/518 [00:13<00:12, 19.76it/s] 52%|█████▏    | 268/518 [00:14<00:12, 19.77it/s] 52%|█████▏    | 271/518 [00:14<00:12, 20.16it/s] 53%|█████▎    | 274/518 [00:14<00:12, 20.20it/s] 53%|█████▎    | 277/518 [00:14<00:12, 20.02it/s] 54%|█████▍    | 280/518 [00:14<00:11, 19.91it/s] 55%|█████▍    | 283/518 [00:14<00:11, 19.78it/s] 55%|█████▌    | 286/518 [00:15<00:11, 19.92it/s] 56%|█████▌    | 289/518 [00:15<00:11, 19.93it/s] 56%|█████▋    | 292/518 [00:15<00:11, 20.04it/s] 57%|█████▋    | 295/518 [00:15<00:11, 20.18it/s] 58%|█████▊    | 298/518 [00:15<00:10, 20.38it/s] 58%|█████▊    | 301/518 [00:15<00:10, 20.27it/s] 59%|█████▊    | 304/518 [00:15<00:10, 20.10it/s] 59%|█████▉    | 307/518 [00:16<00:10, 19.94it/s] 60%|█████▉    | 310/518 [00:16<00:10, 20.13it/s] 60%|██████    | 313/518 [00:16<00:10, 20.12it/s] 61%|██████    | 316/518 [00:16<00:09, 20.26it/s] 62%|██████▏   | 319/518 [00:16<00:09, 20.36it/s] 62%|██████▏   | 322/518 [00:16<00:11, 17.81it/s] 63%|██████▎   | 325/518 [00:17<00:10, 18.49it/s] 63%|██████▎   | 327/518 [00:17<00:10, 18.63it/s] 64%|██████▎   | 329/518 [00:17<00:09, 18.94it/s] 64%|██████▍   | 331/518 [00:17<00:09, 18.91it/s] 64%|██████▍   | 334/518 [00:17<00:09, 19.27it/s] 65%|██████▌   | 337/518 [00:17<00:09, 19.68it/s] 65%|██████▌   | 339/518 [00:17<00:09, 19.72it/s] 66%|██████▌   | 342/518 [00:17<00:08, 19.83it/s] 67%|██████▋   | 345/518 [00:18<00:08, 19.95it/s] 67%|██████▋   | 347/518 [00:18<00:08, 19.89it/s] 68%|██████▊   | 350/518 [00:18<00:08, 20.26it/s] 68%|██████▊   | 353/518 [00:18<00:08, 19.99it/s] 69%|██████▊   | 356/518 [00:18<00:08, 20.03it/s] 69%|██████▉   | 359/518 [00:18<00:08, 19.77it/s] 70%|██████▉   | 361/518 [00:18<00:07, 19.78it/s] 70%|███████   | 364/518 [00:18<00:07, 20.09it/s] 71%|███████   | 367/518 [00:19<00:07, 20.07it/s] 71%|███████▏  | 370/518 [00:19<00:07, 19.97it/s] 72%|███████▏  | 372/518 [00:19<00:07, 19.49it/s] 72%|███████▏  | 375/518 [00:19<00:07, 19.90it/s] 73%|███████▎  | 378/518 [00:19<00:06, 20.08it/s] 74%|███████▎  | 381/518 [00:19<00:06, 20.08it/s] 74%|███████▍  | 384/518 [00:19<00:06, 19.67it/s] 75%|███████▍  | 386/518 [00:20<00:06, 19.54it/s] 75%|███████▍  | 388/518 [00:20<00:06, 19.43it/s] 75%|███████▌  | 390/518 [00:20<00:06, 19.44it/s] 76%|███████▌  | 392/518 [00:20<00:06, 19.44it/s] 76%|███████▌  | 394/518 [00:20<00:06, 19.38it/s] 77%|███████▋  | 397/518 [00:20<00:06, 19.67it/s] 77%|███████▋  | 399/518 [00:20<00:06, 19.73it/s] 77%|███████▋  | 401/518 [00:20<00:06, 19.47it/s] 78%|███████▊  | 404/518 [00:21<00:05, 19.36it/s] 79%|███████▊  | 407/518 [00:21<00:05, 19.69it/s] 79%|███████▉  | 410/518 [00:21<00:05, 19.87it/s] 80%|███████▉  | 412/518 [00:21<00:05, 19.77it/s] 80%|███████▉  | 414/518 [00:21<00:05, 19.73it/s] 81%|████████  | 417/518 [00:21<00:05, 19.76it/s] 81%|████████  | 420/518 [00:21<00:04, 19.71it/s] 81%|████████▏ | 422/518 [00:21<00:04, 19.37it/s] 82%|████████▏ | 424/518 [00:22<00:04, 19.38it/s] 82%|████████▏ | 426/518 [00:22<00:04, 19.33it/s] 83%|████████▎ | 429/518 [00:22<00:04, 19.73it/s] 83%|████████▎ | 431/518 [00:22<00:04, 19.75it/s] 84%|████████▎ | 433/518 [00:22<00:04, 19.45it/s] 84%|████████▍ | 436/518 [00:22<00:04, 19.76it/s] 85%|████████▍ | 438/518 [00:22<00:04, 19.61it/s] 85%|████████▍ | 440/518 [00:22<00:04, 19.40it/s] 85%|████████▌ | 442/518 [00:22<00:03, 19.38it/s] 86%|████████▌ | 444/518 [00:23<00:03, 19.20it/s] 86%|████████▌ | 446/518 [00:23<00:03, 19.41it/s] 87%|████████▋ | 449/518 [00:23<00:03, 19.72it/s] 87%|████████▋ | 451/518 [00:23<00:03, 19.62it/s] 88%|████████▊ | 454/518 [00:23<00:03, 19.82it/s] 88%|████████▊ | 457/518 [00:23<00:03, 20.07it/s] 89%|████████▉ | 460/518 [00:23<00:02, 20.40it/s] 89%|████████▉ | 463/518 [00:23<00:02, 20.34it/s] 90%|████████▉ | 466/518 [00:24<00:02, 20.32it/s] 91%|█████████ | 469/518 [00:24<00:02, 20.27it/s] 91%|█████████ | 472/518 [00:24<00:02, 20.48it/s] 92%|█████████▏| 475/518 [00:24<00:02, 20.48it/s] 92%|█████████▏| 478/518 [00:24<00:01, 20.47it/s] 93%|█████████▎| 481/518 [00:24<00:01, 20.52it/s] 93%|█████████▎| 484/518 [00:25<00:01, 19.99it/s] 94%|█████████▍| 487/518 [00:25<00:01, 20.15it/s] 95%|█████████▍| 490/518 [00:25<00:01, 20.19it/s] 95%|█████████▌| 493/518 [00:25<00:01, 20.10it/s] 96%|█████████▌| 496/518 [00:25<00:01, 20.10it/s] 96%|█████████▋| 499/518 [00:25<00:00, 20.51it/s] 97%|█████████▋| 502/518 [00:25<00:00, 20.53it/s] 97%|█████████▋| 505/518 [00:26<00:00, 20.24it/s] 98%|█████████▊| 508/518 [00:26<00:00, 20.22it/s] 99%|█████████▊| 511/518 [00:26<00:00, 19.94it/s] 99%|█████████▉| 513/518 [00:26<00:00, 19.61it/s]100%|█████████▉| 516/518 [00:26<00:00, 19.91it/s]100%|██████████| 518/518 [00:26<00:00, 19.39it/s]
merge and save to local disk
DEBUG prepare_saving: input dtype from config = bfloat16, type = <class 'str'>
DEBUG prepare_saving: final output_dtype = torch.bfloat16, type = <class 'torch.dtype'>
DEBUG: Found 196 lora_A, 196 lora_B, 196 modules, 196 scaling
DEBUG: Files before save_pretrained: ['special_tokens_map.json', 'chat_template.jinja', 'tokenizer.json', 'tokenizer_config.json']
DEBUG: Files after save_pretrained: ['config.json', 'special_tokens_map.json', 'chat_template.jinja', 'tokenizer.json', 'generation_config.json', 'tokenizer_config.json', 'model.safetensors']
DEBUG: config.json - Size: 0.00 MB
DEBUG: special_tokens_map.json - Size: 0.00 MB
DEBUG: chat_template.jinja - Size: 0.00 MB
DEBUG: tokenizer.json - Size: 16.41 MB
DEBUG: generation_config.json - Size: 0.00 MB
DEBUG: tokenizer_config.json - Size: 0.05 MB
DEBUG: model.safetensors - Size: 2338.53 MB
DEBUG: model.safetensors contains 1611 keys
DEBUG: Saved all keys to debug_saved_keys.txt
Found HuggingFace hub cache directory: /mnt/disks/unslothai/.cache/huggingface/hub
Checking cache directory for required files...
Cache check failed: model-00001-of-00002.safetensors not found in local cache.
Not all required files found in cache. Will proceed with downloading.
Unsloth: Merging weights into 16bit:   0%|          | 0/2 [00:00<?, ?it/s]DEBUG _merge_and_overwrite_lora: output_dtype = torch.bfloat16, type = <class 'torch.dtype'>
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 0
DEBUG: Saving 1 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 0
DEBUG: Saving 2 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.0.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 1
DEBUG: Saving 3 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.0.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 2
DEBUG: Saving 4 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.0.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 3
DEBUG: Saving 5 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 3
DEBUG: Saving 6 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.0.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 4
DEBUG: Saving 7 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.0.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 5
DEBUG: Saving 8 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.0.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 6
DEBUG: Saving 9 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.0.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 7
DEBUG: Saving 10 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 7
DEBUG: Saving 11 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.1.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 8
DEBUG: Saving 12 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.1.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 9
DEBUG: Saving 13 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.1.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 10
DEBUG: Saving 14 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 10
DEBUG: Saving 15 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.1.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 11
DEBUG: Saving 16 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.1.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 12
DEBUG: Saving 17 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.1.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 13
DEBUG: Saving 18 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.1.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 14
DEBUG: Saving 19 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 14
DEBUG: Saving 20 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.10.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 15
DEBUG: Saving 21 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.10.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 16
DEBUG: Saving 22 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.10.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 17
DEBUG: Saving 23 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 17
DEBUG: Saving 24 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.10.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 18
DEBUG: Saving 25 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.10.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 19
DEBUG: Saving 26 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.10.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 20
DEBUG: Saving 27 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.10.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 21
DEBUG: Saving 28 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 21
DEBUG: Saving 29 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.11.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 22
DEBUG: Saving 30 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.11.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 23
DEBUG: Saving 31 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.11.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 24
DEBUG: Saving 32 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 24
DEBUG: Saving 33 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.11.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 25
DEBUG: Saving 34 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.11.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 26
DEBUG: Saving 35 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.11.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 27
DEBUG: Saving 36 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.11.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 28
DEBUG: Saving 37 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 28
DEBUG: Saving 38 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.12.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 29
DEBUG: Saving 39 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.12.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 30
DEBUG: Saving 40 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.12.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 31
DEBUG: Saving 41 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 31
DEBUG: Saving 42 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.12.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 32
DEBUG: Saving 43 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.12.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 33
DEBUG: Saving 44 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.12.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 34
DEBUG: Saving 45 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.12.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 35
DEBUG: Saving 46 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 35
DEBUG: Saving 47 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.13.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 36
DEBUG: Saving 48 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.13.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 37
DEBUG: Saving 49 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.13.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 38
DEBUG: Saving 50 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 38
DEBUG: Saving 51 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.13.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 39
DEBUG: Saving 52 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.13.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 40
DEBUG: Saving 53 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.13.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 41
DEBUG: Saving 54 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.13.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 42
DEBUG: Saving 55 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 42
DEBUG: Saving 56 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.14.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 43
DEBUG: Saving 57 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.14.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 44
DEBUG: Saving 58 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.14.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 45
DEBUG: Saving 59 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 45
DEBUG: Saving 60 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.14.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 46
DEBUG: Saving 61 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.14.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 47
DEBUG: Saving 62 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.14.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 48
DEBUG: Saving 63 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.14.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 49
DEBUG: Saving 64 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 49
DEBUG: Saving 65 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.15.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 50
DEBUG: Saving 66 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.15.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 51
DEBUG: Saving 67 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.15.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 52
DEBUG: Saving 68 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 52
DEBUG: Saving 69 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.15.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 53
DEBUG: Saving 70 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.15.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 54
DEBUG: Saving 71 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.15.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 55
DEBUG: Saving 72 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.15.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 56
DEBUG: Saving 73 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 56
DEBUG: Saving 74 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.16.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 57
DEBUG: Saving 75 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.16.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 58
DEBUG: Saving 76 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.16.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 59
DEBUG: Saving 77 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 59
DEBUG: Saving 78 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.16.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 60
DEBUG: Saving 79 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.16.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 61
DEBUG: Saving 80 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.16.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 62
DEBUG: Saving 81 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.16.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 63
DEBUG: Saving 82 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 63
DEBUG: Saving 83 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.17.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 64
DEBUG: Saving 84 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.17.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 65
DEBUG: Saving 85 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.17.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 66
DEBUG: Saving 86 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 66
DEBUG: Saving 87 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.17.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 67
DEBUG: Saving 88 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.17.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 68
DEBUG: Saving 89 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.17.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 69
DEBUG: Saving 90 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.17.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 70
DEBUG: Saving 91 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 70
DEBUG: Saving 92 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.18.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 71
DEBUG: Saving 93 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.18.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 72
DEBUG: Saving 94 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.18.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 73
DEBUG: Saving 95 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 73
DEBUG: Saving 96 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.18.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 74
DEBUG: Saving 97 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.18.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 75
DEBUG: Saving 98 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.18.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 76
DEBUG: Saving 99 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.18.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 77
DEBUG: Saving 100 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 77
DEBUG: Saving 101 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.19.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 78
DEBUG: Saving 102 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.19.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 79
DEBUG: Saving 103 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.19.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 80
DEBUG: Saving 104 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 80
DEBUG: Saving 105 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.19.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 81
DEBUG: Saving 106 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.19.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 82
DEBUG: Saving 107 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.19.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 83
DEBUG: Saving 108 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.19.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 84
DEBUG: Saving 109 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 84
DEBUG: Saving 110 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.2.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 85
DEBUG: Saving 111 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.2.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 86
DEBUG: Saving 112 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.2.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 87
DEBUG: Saving 113 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 87
DEBUG: Saving 114 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.2.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 88
DEBUG: Saving 115 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.2.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 89
DEBUG: Saving 116 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.2.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 90
DEBUG: Saving 117 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.2.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 91
DEBUG: Saving 118 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.20.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 92
DEBUG: Saving 119 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.20.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 93
DEBUG: Saving 120 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.20.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 94
DEBUG: Saving 121 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.20.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 95
DEBUG: Saving 122 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.20.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 96
DEBUG: Saving 123 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.20.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 97
DEBUG: Saving 124 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 97
DEBUG: Saving 125 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.3.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 98
DEBUG: Saving 126 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.3.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 99
DEBUG: Saving 127 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.3.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 100
DEBUG: Saving 128 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 100
DEBUG: Saving 129 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.3.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 101
DEBUG: Saving 130 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.3.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 102
DEBUG: Saving 131 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.3.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 103
DEBUG: Saving 132 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.3.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 104
DEBUG: Saving 133 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 104
DEBUG: Saving 134 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.4.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 105
DEBUG: Saving 135 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.4.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 106
DEBUG: Saving 136 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.4.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 107
DEBUG: Saving 137 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 107
DEBUG: Saving 138 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.4.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 108
DEBUG: Saving 139 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.4.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 109
DEBUG: Saving 140 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.4.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 110
DEBUG: Saving 141 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.4.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 111
DEBUG: Saving 142 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 111
DEBUG: Saving 143 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.5.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 112
DEBUG: Saving 144 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.5.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 113
DEBUG: Saving 145 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.5.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 114
DEBUG: Saving 146 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 114
DEBUG: Saving 147 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.5.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 115
DEBUG: Saving 148 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.5.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 116
DEBUG: Saving 149 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.5.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 117
DEBUG: Saving 150 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.5.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 118
DEBUG: Saving 151 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 118
DEBUG: Saving 152 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.6.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 119
DEBUG: Saving 153 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.6.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 120
DEBUG: Saving 154 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.6.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 121
DEBUG: Saving 155 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 121
DEBUG: Saving 156 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.6.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 122
DEBUG: Saving 157 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.6.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 123
DEBUG: Saving 158 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.6.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 124
DEBUG: Saving 159 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.6.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 125
DEBUG: Saving 160 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 125
DEBUG: Saving 161 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.7.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 126
DEBUG: Saving 162 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.7.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 127
DEBUG: Saving 163 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.7.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 128
DEBUG: Saving 164 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 128
DEBUG: Saving 165 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.7.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 129
DEBUG: Saving 166 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.7.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 130
DEBUG: Saving 167 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.7.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 131
DEBUG: Saving 168 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.7.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 132
DEBUG: Saving 169 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 132
DEBUG: Saving 170 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.8.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 133
DEBUG: Saving 171 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.8.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 134
DEBUG: Saving 172 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.8.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 135
DEBUG: Saving 173 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 135
DEBUG: Saving 174 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.8.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 136
DEBUG: Saving 175 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.8.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 137
DEBUG: Saving 176 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.8.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 138
DEBUG: Saving 177 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.8.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 139
DEBUG: Saving 178 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 139
DEBUG: Saving 179 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.9.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 140
DEBUG: Saving 180 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.9.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 141
DEBUG: Saving 181 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.9.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 142
DEBUG: Saving 182 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 142
DEBUG: Saving 183 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.9.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 143
DEBUG: Saving 184 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.9.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
Unsloth: Merging weights into 16bit:  50%|█████     | 1/2 [00:25<00:25, 25.28s/it]DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 144
DEBUG: Saving 185 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.9.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 145
DEBUG: Saving 186 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.9.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 146
DEBUG: Saving 187 tensors to ./unsloth_out/merged_llama_text_model/model-00001-of-00002.safetensors
DEBUG _merge_and_overwrite_lora: output_dtype = torch.bfloat16, type = <class 'torch.dtype'>
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 0
DEBUG: Saving 1 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.20.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 1
DEBUG: Saving 2 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 1
DEBUG: Saving 3 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 1
DEBUG: Saving 4 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.21.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 2
DEBUG: Saving 5 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.21.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 3
DEBUG: Saving 6 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.21.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 4
DEBUG: Saving 7 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 4
DEBUG: Saving 8 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.21.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 5
DEBUG: Saving 9 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.21.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 6
DEBUG: Saving 10 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.21.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 7
DEBUG: Saving 11 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.21.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 8
DEBUG: Saving 12 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 8
DEBUG: Saving 13 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.22.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 9
DEBUG: Saving 14 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.22.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 10
DEBUG: Saving 15 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.22.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 11
DEBUG: Saving 16 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 11
DEBUG: Saving 17 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.22.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 12
DEBUG: Saving 18 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.22.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 13
DEBUG: Saving 19 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.22.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 14
DEBUG: Saving 20 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.22.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 15
DEBUG: Saving 21 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 15
DEBUG: Saving 22 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.23.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 16
DEBUG: Saving 23 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.23.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 17
DEBUG: Saving 24 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.23.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 18
DEBUG: Saving 25 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 18
DEBUG: Saving 26 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.23.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 19
DEBUG: Saving 27 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.23.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 20
DEBUG: Saving 28 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.23.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 21
DEBUG: Saving 29 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.23.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 22
DEBUG: Saving 30 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 22
DEBUG: Saving 31 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.24.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 23
DEBUG: Saving 32 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.24.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 24
DEBUG: Saving 33 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.24.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 25
DEBUG: Saving 34 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 25
DEBUG: Saving 35 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.24.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 26
DEBUG: Saving 36 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.24.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 27
DEBUG: Saving 37 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.24.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 28
DEBUG: Saving 38 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.24.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 29
DEBUG: Saving 39 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 29
DEBUG: Saving 40 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.25.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 30
DEBUG: Saving 41 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.25.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 31
DEBUG: Saving 42 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.25.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 32
DEBUG: Saving 43 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 32
DEBUG: Saving 44 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.25.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 33
DEBUG: Saving 45 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.25.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 34
DEBUG: Saving 46 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.25.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 35
DEBUG: Saving 47 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.25.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 36
DEBUG: Saving 48 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 36
DEBUG: Saving 49 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.26.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 37
DEBUG: Saving 50 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.26.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 38
DEBUG: Saving 51 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.26.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 39
DEBUG: Saving 52 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 39
DEBUG: Saving 53 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.26.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:32<00:00, 14.68s/it]Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:32<00:00, 16.27s/it]
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 40
DEBUG: Saving 54 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.26.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 41
DEBUG: Saving 55 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.26.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 42
DEBUG: Saving 56 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.26.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 43
DEBUG: Saving 57 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 43
DEBUG: Saving 58 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.27.mlp.down_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 8192])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 44
DEBUG: Saving 59 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.27.mlp.gate_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 45
DEBUG: Saving 60 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.27.mlp.up_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([8192, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 46
DEBUG: Saving 61 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 46
DEBUG: Saving 62 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.27.self_attn.k_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 47
DEBUG: Saving 63 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.27.self_attn.o_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 48
DEBUG: Saving 64 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.27.self_attn.q_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([3072, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 49
DEBUG: Saving 65 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Merging LoRA for key model.layers.27.self_attn.v_proj.weight
DEBUG: After merge, W dtype = torch.float32, shape = torch.Size([1024, 3072])
DEBUG: Before output_dtype conversion: W.dtype = torch.float32
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 50
DEBUG: Saving 66 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
DEBUG: Before output_dtype conversion: W.dtype = torch.bfloat16
DEBUG: After conversion to output_dtype: W_converted.dtype = torch.bfloat16
DEBUG: After reload from temp file: W.dtype = torch.bfloat16
DEBUG: Total merged keys: 50
DEBUG: Saving 67 tensors to ./unsloth_out/merged_llama_text_model/model-00002-of-00002.safetensors
Loading merged model in 4 bit for perplexity test
==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.0. vLLM: 0.10.1.1.
   \\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.209 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Traceback (most recent call last):
  File "/home/support/unsloth/tests/saving/language_models/test_merge_model_perplexity_llama-3.2.py", line 186, in <module>
    merged_model, merged_tokenizer = FastLanguageModel.from_pretrained(
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/site-packages/unsloth/models/loader.py", line 404, in from_pretrained
    model, tokenizer = dispatch_model.from_pretrained(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/site-packages/unsloth/models/llama.py", line 2048, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/site-packages/transformers/modeling_utils.py", line 288, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5176, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5711, in _load_pretrained_model
    warner(
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/logging/__init__.py", line 1501, in warning
    self._log(WARNING, msg, args, **kwargs)
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/logging/__init__.py", line 1634, in _log
    self.handle(record)
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/logging/__init__.py", line 1644, in handle
    self.callHandlers(record)
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/logging/__init__.py", line 1706, in callHandlers
    hdlr.handle(record)
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/logging/__init__.py", line 978, in handle
    self.emit(record)
  File "/home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/site-packages/unsloth/models/_utils.py", line 331, in emit
    raise Exception(
Exception: Unsloth: Critical error since some weights are not initialized.
Please try updating Unsloth, transformers and timm via:
`pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo transformers timm`
<LogRecord: transformers.modeling_utils, 30, /home/support/.local/share/mamba/envs/unsloth_test_sep4/lib/python3.11/site-packages/transformers/modeling_utils.py, 5711, "Some weights of the model checkpoint at ./unsloth_out/merged_llama_text_model were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.base_layer.weight', 'model.layers.0.mlp.down_proj.base_layer.weight.absmax', 'model.layers.0.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.0.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.0.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.down_proj.lora_A.default.weight', 'model.layers.0.mlp.down_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.0.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.0.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.0.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight', 'model.layers.0.mlp.up_proj.base_layer.weight', 'model.layers.0.mlp.up_proj.base_layer.weight.absmax', 'model.layers.0.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.0.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.0.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.up_proj.lora_A.default.weight', 'model.layers.0.mlp.up_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.mlp.down_proj.base_layer.weight', 'model.layers.1.mlp.down_proj.lora_A.default.weight', 'model.layers.1.mlp.down_proj.lora_B.default.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight', 'model.layers.1.mlp.gate_proj.lora_A.default.weight', 'model.layers.1.mlp.gate_proj.lora_B.default.weight', 'model.layers.1.mlp.up_proj.base_layer.weight', 'model.layers.1.mlp.up_proj.lora_A.default.weight', 'model.layers.1.mlp.up_proj.lora_B.default.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.mlp.down_proj.base_layer.weight', 'model.layers.10.mlp.down_proj.base_layer.weight.absmax', 'model.layers.10.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.10.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.10.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.down_proj.lora_A.default.weight', 'model.layers.10.mlp.down_proj.lora_B.default.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.10.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.10.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.10.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.gate_proj.lora_A.default.weight', 'model.layers.10.mlp.gate_proj.lora_B.default.weight', 'model.layers.10.mlp.up_proj.base_layer.weight', 'model.layers.10.mlp.up_proj.base_layer.weight.absmax', 'model.layers.10.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.10.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.10.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.up_proj.lora_A.default.weight', 'model.layers.10.mlp.up_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.mlp.down_proj.base_layer.weight', 'model.layers.11.mlp.down_proj.base_layer.weight.absmax', 'model.layers.11.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.11.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.11.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.down_proj.lora_A.default.weight', 'model.layers.11.mlp.down_proj.lora_B.default.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.11.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.11.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.11.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.gate_proj.lora_A.default.weight', 'model.layers.11.mlp.gate_proj.lora_B.default.weight', 'model.layers.11.mlp.up_proj.base_layer.weight', 'model.layers.11.mlp.up_proj.base_layer.weight.absmax', 'model.layers.11.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.11.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.11.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.up_proj.lora_A.default.weight', 'model.layers.11.mlp.up_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.mlp.down_proj.base_layer.weight', 'model.layers.12.mlp.down_proj.base_layer.weight.absmax', 'model.layers.12.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.12.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.12.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.down_proj.lora_A.default.weight', 'model.layers.12.mlp.down_proj.lora_B.default.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.12.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.12.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.12.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.gate_proj.lora_A.default.weight', 'model.layers.12.mlp.gate_proj.lora_B.default.weight', 'model.layers.12.mlp.up_proj.base_layer.weight', 'model.layers.12.mlp.up_proj.base_layer.weight.absmax', 'model.layers.12.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.12.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.12.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.up_proj.lora_A.default.weight', 'model.layers.12.mlp.up_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.mlp.down_proj.base_layer.weight', 'model.layers.13.mlp.down_proj.base_layer.weight.absmax', 'model.layers.13.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.13.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.13.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.down_proj.lora_A.default.weight', 'model.layers.13.mlp.down_proj.lora_B.default.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.13.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.13.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.13.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.gate_proj.lora_A.default.weight', 'model.layers.13.mlp.gate_proj.lora_B.default.weight', 'model.layers.13.mlp.up_proj.base_layer.weight', 'model.layers.13.mlp.up_proj.base_layer.weight.absmax', 'model.layers.13.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.13.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.13.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.up_proj.lora_A.default.weight', 'model.layers.13.mlp.up_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.mlp.down_proj.base_layer.weight', 'model.layers.14.mlp.down_proj.base_layer.weight.absmax', 'model.layers.14.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.14.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.14.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.down_proj.lora_A.default.weight', 'model.layers.14.mlp.down_proj.lora_B.default.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.14.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.14.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.14.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.gate_proj.lora_A.default.weight', 'model.layers.14.mlp.gate_proj.lora_B.default.weight', 'model.layers.14.mlp.up_proj.base_layer.weight', 'model.layers.14.mlp.up_proj.base_layer.weight.absmax', 'model.layers.14.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.14.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.14.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.up_proj.lora_A.default.weight', 'model.layers.14.mlp.up_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.mlp.down_proj.base_layer.weight', 'model.layers.15.mlp.down_proj.base_layer.weight.absmax', 'model.layers.15.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.15.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.15.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.down_proj.lora_A.default.weight', 'model.layers.15.mlp.down_proj.lora_B.default.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.15.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.15.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.15.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.gate_proj.lora_A.default.weight', 'model.layers.15.mlp.gate_proj.lora_B.default.weight', 'model.layers.15.mlp.up_proj.base_layer.weight', 'model.layers.15.mlp.up_proj.base_layer.weight.absmax', 'model.layers.15.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.15.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.15.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.up_proj.lora_A.default.weight', 'model.layers.15.mlp.up_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.mlp.down_proj.base_layer.weight', 'model.layers.16.mlp.down_proj.base_layer.weight.absmax', 'model.layers.16.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.16.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.16.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.down_proj.lora_A.default.weight', 'model.layers.16.mlp.down_proj.lora_B.default.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.16.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.16.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.16.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.gate_proj.lora_A.default.weight', 'model.layers.16.mlp.gate_proj.lora_B.default.weight', 'model.layers.16.mlp.up_proj.base_layer.weight', 'model.layers.16.mlp.up_proj.base_layer.weight.absmax', 'model.layers.16.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.16.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.16.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.up_proj.lora_A.default.weight', 'model.layers.16.mlp.up_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.mlp.down_proj.base_layer.weight', 'model.layers.17.mlp.down_proj.base_layer.weight.absmax', 'model.layers.17.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.17.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.17.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.down_proj.lora_A.default.weight', 'model.layers.17.mlp.down_proj.lora_B.default.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.17.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.17.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.17.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.gate_proj.lora_A.default.weight', 'model.layers.17.mlp.gate_proj.lora_B.default.weight', 'model.layers.17.mlp.up_proj.base_layer.weight', 'model.layers.17.mlp.up_proj.base_layer.weight.absmax', 'model.layers.17.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.17.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.17.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.up_proj.lora_A.default.weight', 'model.layers.17.mlp.up_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.mlp.down_proj.base_layer.weight', 'model.layers.18.mlp.down_proj.base_layer.weight.absmax', 'model.layers.18.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.18.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.18.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.18.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.mlp.down_proj.lora_A.default.weight', 'model.layers.18.mlp.down_proj.lora_B.default.weight', 'model.layers.18.mlp.gate_proj.base_layer.weight', 'model.layers.18.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.18.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.18.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.18.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.18.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.mlp.gate_proj.lora_A.default.weight', 'model.layers.18.mlp.gate_proj.lora_B.default.weight', 'model.layers.18.mlp.up_proj.base_layer.weight', 'model.layers.18.mlp.up_proj.base_layer.weight.absmax', 'model.layers.18.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.18.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.18.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.18.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.mlp.up_proj.lora_A.default.weight', 'model.layers.18.mlp.up_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.base_layer.weight', 'model.layers.18.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.18.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.18.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.18.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.18.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.o_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.18.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.18.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.18.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.18.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.mlp.down_proj.base_layer.weight', 'model.layers.19.mlp.down_proj.base_layer.weight.absmax', 'model.layers.19.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.19.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.19.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.19.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.mlp.down_proj.lora_A.default.weight', 'model.layers.19.mlp.down_proj.lora_B.default.weight', 'model.layers.19.mlp.gate_proj.base_layer.weight', 'model.layers.19.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.19.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.19.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.19.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.19.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.mlp.gate_proj.lora_A.default.weight', 'model.layers.19.mlp.gate_proj.lora_B.default.weight', 'model.layers.19.mlp.up_proj.base_layer.weight', 'model.layers.19.mlp.up_proj.base_layer.weight.absmax', 'model.layers.19.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.19.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.19.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.19.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.mlp.up_proj.lora_A.default.weight', 'model.layers.19.mlp.up_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.base_layer.weight', 'model.layers.19.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.19.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.19.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.19.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.19.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.o_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.19.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.19.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.19.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.19.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.mlp.down_proj.base_layer.weight', 'model.layers.2.mlp.down_proj.base_layer.weight.absmax', 'model.layers.2.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.2.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.2.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.down_proj.lora_A.default.weight', 'model.layers.2.mlp.down_proj.lora_B.default.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.2.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.2.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.2.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.gate_proj.lora_A.default.weight', 'model.layers.2.mlp.gate_proj.lora_B.default.weight', 'model.layers.2.mlp.up_proj.base_layer.weight', 'model.layers.2.mlp.up_proj.base_layer.weight.absmax', 'model.layers.2.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.2.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.2.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.up_proj.lora_A.default.weight', 'model.layers.2.mlp.up_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.mlp.down_proj.base_layer.weight', 'model.layers.20.mlp.down_proj.base_layer.weight.absmax', 'model.layers.20.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.20.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.20.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.20.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.mlp.down_proj.lora_A.default.weight', 'model.layers.20.mlp.down_proj.lora_B.default.weight', 'model.layers.20.mlp.gate_proj.base_layer.weight', 'model.layers.20.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.20.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.20.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.20.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.20.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.mlp.gate_proj.lora_A.default.weight', 'model.layers.20.mlp.gate_proj.lora_B.default.weight', 'model.layers.20.mlp.up_proj.base_layer.weight', 'model.layers.20.mlp.up_proj.base_layer.weight.absmax', 'model.layers.20.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.20.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.20.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.20.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.mlp.up_proj.lora_A.default.weight', 'model.layers.20.mlp.up_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.20.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.20.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.k_proj.lora_A.default.weight', 'model.layers.20.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.20.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.20.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.o_proj.lora_A.default.weight', 'model.layers.20.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.20.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.20.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.20.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.20.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.mlp.down_proj.base_layer.weight', 'model.layers.21.mlp.down_proj.base_layer.weight.absmax', 'model.layers.21.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.21.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.21.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.21.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.mlp.down_proj.lora_A.default.weight', 'model.layers.21.mlp.down_proj.lora_B.default.weight', 'model.layers.21.mlp.gate_proj.base_layer.weight', 'model.layers.21.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.21.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.21.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.21.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.21.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.mlp.gate_proj.lora_A.default.weight', 'model.layers.21.mlp.gate_proj.lora_B.default.weight', 'model.layers.21.mlp.up_proj.base_layer.weight', 'model.layers.21.mlp.up_proj.base_layer.weight.absmax', 'model.layers.21.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.21.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.21.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.21.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.mlp.up_proj.lora_A.default.weight', 'model.layers.21.mlp.up_proj.lora_B.default.weight', 'model.layers.21.self_attn.k_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.21.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.21.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.k_proj.lora_A.default.weight', 'model.layers.21.self_attn.k_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.21.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.21.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.o_proj.lora_A.default.weight', 'model.layers.21.self_attn.o_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.21.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.21.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.21.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.21.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.mlp.down_proj.base_layer.weight', 'model.layers.22.mlp.down_proj.base_layer.weight.absmax', 'model.layers.22.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.22.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.22.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.22.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.mlp.down_proj.lora_A.default.weight', 'model.layers.22.mlp.down_proj.lora_B.default.weight', 'model.layers.22.mlp.gate_proj.base_layer.weight', 'model.layers.22.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.22.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.22.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.22.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.22.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.mlp.gate_proj.lora_A.default.weight', 'model.layers.22.mlp.gate_proj.lora_B.default.weight', 'model.layers.22.mlp.up_proj.base_layer.weight', 'model.layers.22.mlp.up_proj.base_layer.weight.absmax', 'model.layers.22.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.22.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.22.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.22.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.mlp.up_proj.lora_A.default.weight', 'model.layers.22.mlp.up_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.base_layer.weight', 'model.layers.22.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.22.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.22.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.k_proj.lora_B.default.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.22.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.22.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.o_proj.lora_A.default.weight', 'model.layers.22.self_attn.o_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.22.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.22.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.22.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.22.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.mlp.down_proj.base_layer.weight', 'model.layers.23.mlp.down_proj.base_layer.weight.absmax', 'model.layers.23.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.23.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.23.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.23.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.mlp.down_proj.lora_A.default.weight', 'model.layers.23.mlp.down_proj.lora_B.default.weight', 'model.layers.23.mlp.gate_proj.base_layer.weight', 'model.layers.23.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.23.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.23.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.23.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.23.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.mlp.gate_proj.lora_A.default.weight', 'model.layers.23.mlp.gate_proj.lora_B.default.weight', 'model.layers.23.mlp.up_proj.base_layer.weight', 'model.layers.23.mlp.up_proj.base_layer.weight.absmax', 'model.layers.23.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.23.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.23.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.23.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.mlp.up_proj.lora_A.default.weight', 'model.layers.23.mlp.up_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.base_layer.weight', 'model.layers.23.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.23.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.23.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.k_proj.lora_A.default.weight', 'model.layers.23.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.23.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.23.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.o_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.23.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.23.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.23.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.23.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.mlp.down_proj.base_layer.weight', 'model.layers.24.mlp.down_proj.base_layer.weight.absmax', 'model.layers.24.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.24.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.24.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.24.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.mlp.down_proj.lora_A.default.weight', 'model.layers.24.mlp.down_proj.lora_B.default.weight', 'model.layers.24.mlp.gate_proj.base_layer.weight', 'model.layers.24.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.24.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.24.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.24.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.24.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.mlp.gate_proj.lora_A.default.weight', 'model.layers.24.mlp.gate_proj.lora_B.default.weight', 'model.layers.24.mlp.up_proj.base_layer.weight', 'model.layers.24.mlp.up_proj.base_layer.weight.absmax', 'model.layers.24.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.24.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.24.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.24.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.mlp.up_proj.lora_A.default.weight', 'model.layers.24.mlp.up_proj.lora_B.default.weight', 'model.layers.24.self_attn.k_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.24.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.24.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.k_proj.lora_A.default.weight', 'model.layers.24.self_attn.k_proj.lora_B.default.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.24.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.24.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.o_proj.lora_A.default.weight', 'model.layers.24.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.24.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.24.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.24.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.24.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.mlp.down_proj.base_layer.weight', 'model.layers.25.mlp.down_proj.base_layer.weight.absmax', 'model.layers.25.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.25.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.25.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.25.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.mlp.down_proj.lora_A.default.weight', 'model.layers.25.mlp.down_proj.lora_B.default.weight', 'model.layers.25.mlp.gate_proj.base_layer.weight', 'model.layers.25.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.25.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.25.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.25.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.25.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.mlp.gate_proj.lora_A.default.weight', 'model.layers.25.mlp.gate_proj.lora_B.default.weight', 'model.layers.25.mlp.up_proj.base_layer.weight', 'model.layers.25.mlp.up_proj.base_layer.weight.absmax', 'model.layers.25.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.25.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.25.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.25.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.mlp.up_proj.lora_A.default.weight', 'model.layers.25.mlp.up_proj.lora_B.default.weight', 'model.layers.25.self_attn.k_proj.base_layer.weight', 'model.layers.25.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.25.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.25.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.k_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_B.default.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.25.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.25.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.o_proj.lora_A.default.weight', 'model.layers.25.self_attn.o_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.25.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.25.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.25.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.25.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.mlp.down_proj.base_layer.weight', 'model.layers.26.mlp.down_proj.base_layer.weight.absmax', 'model.layers.26.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.26.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.26.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.26.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.mlp.down_proj.lora_A.default.weight', 'model.layers.26.mlp.down_proj.lora_B.default.weight', 'model.layers.26.mlp.gate_proj.base_layer.weight', 'model.layers.26.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.26.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.26.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.26.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.26.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.mlp.gate_proj.lora_A.default.weight', 'model.layers.26.mlp.gate_proj.lora_B.default.weight', 'model.layers.26.mlp.up_proj.base_layer.weight', 'model.layers.26.mlp.up_proj.base_layer.weight.absmax', 'model.layers.26.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.26.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.26.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.26.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.mlp.up_proj.lora_A.default.weight', 'model.layers.26.mlp.up_proj.lora_B.default.weight', 'model.layers.26.self_attn.k_proj.base_layer.weight', 'model.layers.26.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.26.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.26.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.k_proj.lora_A.default.weight', 'model.layers.26.self_attn.k_proj.lora_B.default.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.26.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.26.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.o_proj.lora_A.default.weight', 'model.layers.26.self_attn.o_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.26.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.26.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.26.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.26.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.mlp.down_proj.base_layer.weight', 'model.layers.27.mlp.down_proj.base_layer.weight.absmax', 'model.layers.27.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.27.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.27.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.27.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.mlp.down_proj.lora_A.default.weight', 'model.layers.27.mlp.down_proj.lora_B.default.weight', 'model.layers.27.mlp.gate_proj.base_layer.weight', 'model.layers.27.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.27.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.27.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.27.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.27.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.mlp.gate_proj.lora_A.default.weight', 'model.layers.27.mlp.gate_proj.lora_B.default.weight', 'model.layers.27.mlp.up_proj.base_layer.weight', 'model.layers.27.mlp.up_proj.base_layer.weight.absmax', 'model.layers.27.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.27.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.27.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.27.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.mlp.up_proj.lora_A.default.weight', 'model.layers.27.mlp.up_proj.lora_B.default.weight', 'model.layers.27.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.27.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.27.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.k_proj.lora_A.default.weight', 'model.layers.27.self_attn.k_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.27.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.27.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.o_proj.lora_A.default.weight', 'model.layers.27.self_attn.o_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.27.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.27.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.27.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.27.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.mlp.down_proj.base_layer.weight', 'model.layers.3.mlp.down_proj.base_layer.weight.absmax', 'model.layers.3.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.3.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.3.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.down_proj.lora_A.default.weight', 'model.layers.3.mlp.down_proj.lora_B.default.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.3.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.3.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.3.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.gate_proj.lora_A.default.weight', 'model.layers.3.mlp.gate_proj.lora_B.default.weight', 'model.layers.3.mlp.up_proj.base_layer.weight', 'model.layers.3.mlp.up_proj.base_layer.weight.absmax', 'model.layers.3.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.3.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.3.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.up_proj.lora_A.default.weight', 'model.layers.3.mlp.up_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.mlp.down_proj.base_layer.weight', 'model.layers.4.mlp.down_proj.base_layer.weight.absmax', 'model.layers.4.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.4.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.4.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.down_proj.lora_A.default.weight', 'model.layers.4.mlp.down_proj.lora_B.default.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.4.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.4.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.4.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.gate_proj.lora_A.default.weight', 'model.layers.4.mlp.gate_proj.lora_B.default.weight', 'model.layers.4.mlp.up_proj.base_layer.weight', 'model.layers.4.mlp.up_proj.base_layer.weight.absmax', 'model.layers.4.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.4.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.4.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.up_proj.lora_A.default.weight', 'model.layers.4.mlp.up_proj.lora_B.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.mlp.down_proj.base_layer.weight', 'model.layers.5.mlp.down_proj.base_layer.weight.absmax', 'model.layers.5.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.5.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.5.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.down_proj.lora_A.default.weight', 'model.layers.5.mlp.down_proj.lora_B.default.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.5.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.5.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.5.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.gate_proj.lora_A.default.weight', 'model.layers.5.mlp.gate_proj.lora_B.default.weight', 'model.layers.5.mlp.up_proj.base_layer.weight', 'model.layers.5.mlp.up_proj.base_layer.weight.absmax', 'model.layers.5.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.5.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.5.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.up_proj.lora_A.default.weight', 'model.layers.5.mlp.up_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.mlp.down_proj.base_layer.weight', 'model.layers.6.mlp.down_proj.base_layer.weight.absmax', 'model.layers.6.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.6.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.6.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.down_proj.lora_A.default.weight', 'model.layers.6.mlp.down_proj.lora_B.default.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.6.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.6.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.6.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.gate_proj.lora_A.default.weight', 'model.layers.6.mlp.gate_proj.lora_B.default.weight', 'model.layers.6.mlp.up_proj.base_layer.weight', 'model.layers.6.mlp.up_proj.base_layer.weight.absmax', 'model.layers.6.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.6.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.6.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.up_proj.lora_A.default.weight', 'model.layers.6.mlp.up_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.mlp.down_proj.base_layer.weight', 'model.layers.7.mlp.down_proj.base_layer.weight.absmax', 'model.layers.7.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.7.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.7.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.down_proj.lora_A.default.weight', 'model.layers.7.mlp.down_proj.lora_B.default.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.7.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.7.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.7.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.gate_proj.lora_A.default.weight', 'model.layers.7.mlp.gate_proj.lora_B.default.weight', 'model.layers.7.mlp.up_proj.base_layer.weight', 'model.layers.7.mlp.up_proj.base_layer.weight.absmax', 'model.layers.7.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.7.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.7.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.up_proj.lora_A.default.weight', 'model.layers.7.mlp.up_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.mlp.down_proj.base_layer.weight', 'model.layers.8.mlp.down_proj.base_layer.weight.absmax', 'model.layers.8.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.8.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.8.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.down_proj.lora_A.default.weight', 'model.layers.8.mlp.down_proj.lora_B.default.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.8.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.8.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.8.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.gate_proj.lora_A.default.weight', 'model.layers.8.mlp.gate_proj.lora_B.default.weight', 'model.layers.8.mlp.up_proj.base_layer.weight', 'model.layers.8.mlp.up_proj.base_layer.weight.absmax', 'model.layers.8.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.8.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.8.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.up_proj.lora_A.default.weight', 'model.layers.8.mlp.up_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.mlp.down_proj.base_layer.weight', 'model.layers.9.mlp.down_proj.base_layer.weight.absmax', 'model.layers.9.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.9.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.9.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.down_proj.lora_A.default.weight', 'model.layers.9.mlp.down_proj.lora_B.default.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.9.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.9.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.9.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.gate_proj.lora_A.default.weight', 'model.layers.9.mlp.gate_proj.lora_B.default.weight', 'model.layers.9.mlp.up_proj.base_layer.weight', 'model.layers.9.mlp.up_proj.base_layer.weight.absmax', 'model.layers.9.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.9.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.9.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.up_proj.lora_A.default.weight', 'model.layers.9.mlp.up_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).">
